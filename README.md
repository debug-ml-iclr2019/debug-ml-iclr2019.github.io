# Overview
Machine learning (ML) models are increasingly being employed to make highly consequential decisions pertaining to [employment](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G), [bail](https://www.nber.org/papers/w23180), [parole](http://advances.sciencemag.org/content/4/1/eaao5580), and [lending](https://yjolt.org/credit-scoring-era-big-data). While such models can learn from large amounts of data and are often very scalable, their applicability is limited by certain safety challenges. A key challenge is to be able to identify and correct systematic patterns of mistakes made by ML models before deploying them in the real world. 

The goal of this workshop is to bring together researchers and practitioners interested in research problems and questions pertaining to the debugging of machine learning models. For the first edition of this workshop, we intend to focus on research that approaches the problem of debugging ML models from the following perspectives: 

- Interpretable and explainable ML
- Formal methods and program verification
- Visualization and human factors
- Security and Robustness in ML

By bringing together researchers and practitioners working in the aforementioned research areas, we hope to address several key questions pertaining to model debugging (some of which are highlighted below) and facilitate an insightful discussion about the strengths and weaknesses of existing approaches:

- How can interpretable models and techniques aid us in effectively debugging ML models?
- Are existing program verification frameworks readily applicable to ML models? If not, what are the gaps that exist and how do we bridge them?
- What kind of visualization techniques would be most effective in exposing vulnerabilities of ML models? 
- How can we leverage techniques and insights from the emerging area of adversarial machine learning to debug ML models?
- What are some of the effective strategies for involving humans in the loop for debugging ML models?
- Can we take cues from statistical considerations such as multiple testing, uncertainty, and false discovery rate control in order to ensure that debugging methodologies and tools actually detect ‘true’ errors of the model being examined?


# Call for Papers and Submission Instructions
We invite researchers to submit anonymous extended abstracts of up to 4 pages (excluding references) in the ICLR template. More details to come.

Submit on TBA.


# Important Dates

- Submission Deadline: TBD
- Notification: TBD
- Workshop: Mon May 6th 09:00 AM -- 06:00 PM @ Room R3

# Organizers
- [Himabindu Lakkaraju](https://web.stanford.edu/~himalv/) (Harvard University)
- [Sarah Tan](https://shftan.github.io/) (Cornell University and UCSF)
- [Julius Adebayo](http://juliusadebayo.com/) (MIT)
- [Jacob Steinhardt](https://cs.stanford.edu/~jsteinhardt/) (Open Philanthropy Project and at OpenAI)
- [Rich Caruana](https://www.microsoft.com/en-us/research/people/rcaruana/) (Microsoft Research)


Please email [debugging.ml@gmail.com](mailto:debugging.ml@gmail.com) with any questions.


# Tentative Schedule

| Time | Event |
| --- | --- |
| 8:50 - 9:00 | Introductory Remarks from Organizers |
| 9:00 - 9:30 | Invited talk 1 |
| 9:30 - 10:00 | 2 contributed oral talks |
| 10:00 - 10:15 | Break |
| 10:15 - 10:45 | Invited talk 2 |
| 10:45 - 11:00 | 3 spotlight contributed talks (5 mins each) |
| 11.00 - 12.00 | First Poster session |
| 12.00 - 1.15 | Lunch |
| 1.15 - 1.45 | Opinion piece: Can model Interpretability help with debugging? | 
| 1.45 pm - 2.00 | Discussion (for opinion piece) | 
| 2.00 pm - 2.30 | Invited talk 3 (30 mins talk including 5 mins questions) |
| 2.30 pm - 2.45 | Break |
| 2.45 pm - 3.30 | Panel (see discussion of panel below) |
| 3.30 pm - 4.00 | 2 contributed oral talks (15 mins each, including questions) |
| 4.00 pm - 4.30 | Invited talk 4 (30 mins talk including 5 mins questions) |
| 4.30 pm - 5.30 | Poster session |